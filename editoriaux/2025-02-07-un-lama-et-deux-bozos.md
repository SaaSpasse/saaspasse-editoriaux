What's up folks,

Merci d'Ãªtre lÃ . Lâ€™Ã©dito dâ€™abordâ€”les nouvelles aprÃ¨s.

Câ€™est la premiÃ¨re fois de ma vie quâ€™une dÃ©mo fonctionne du premier coup!

On venait de faire rouler Llama 3.2. Rendu Ã  tester DeepSeek-R1, quand mi-*promptâ€¦*

**___full laptop crash___**

Coincidence? JE NE CROIS PAS, CCP!

Bref, Douville dâ€™[Apollo13](https://www.saaspasse.com/partenaires/apollo13?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos) mâ€™a appris Ã  rouler des modÃ¨les dâ€™AI aux milliards de paramÃ¨tres direct sur ma machine. Dans un UI trÃ¨s utilisateur-amical. **Pour 0 piastre**.

Bonus : ta data reste sur ton ordi. MÃªme pas besoin dâ€™Ãªtre connectÃ© sur le Ternet.

TsÃ©, les abonnements Ã  ChatGPT, Claude, Perplexity, etc., sâ€™accumulent. Et ils tâ€™imposent souvent des limites dâ€™utilisation. Ce qui aggrave **immÃ©diatement** la situation dÃ©jÃ  **super Ã©prouvante** dâ€™Ãªtre un *tech worker* en Occident sur son laptop (avec lattÃ©).

Peut-Ãªtre es-tu dans la position du founder avec exit rÃ©cent qui mâ€™a dit :

Le truc pour les limites d'utilisation, c'est **trois** comptes Claude pro payants.

Mais si tâ€™es comme moi, tu comptes tes cents sans dÃ©penser sur des folies comme un abonnement annuel Ã  Midjourney pour faire trois images par annÃ©e (je mens, je lâ€™ai fait).

Sans coder ni dÃ©penser un dollar, tu peux*Â *rouler un â€œChatGPTâ€ sur ton laptop. Voici comment, vidÃ©o en bonus ğŸ‘‡

### Pourquoi rouler un LLM sur ta machine direct?

Il fait f**king frette ces temps-ci, donc je commencerai par citer [Vincent Bernard](https://www.saaspasse.com/episode/episode-66-vincent-bernard-balade-genai-a-dos-de-licorne?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos), directeur R&D chez Coveo :

Blague Ã  part, Ã§a peut te manger de la RAM pas mal Ã§a de lâ€™air. Donc si tâ€™es encore en train dâ€™user ton vieux Macbook Ã  lâ€™os, *well*â€¦ lis une des autres Ã©ditions de lâ€™infolettreâ€”sont toutes bonnes.

OU procure-toi un nouveau Macbook via [notre partenariat SaaSpasse x Apple](https://www.youtube.com/watch?v=dQw4w9WgXcQ&utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos)*.
*dis-moi que tâ€™as pas cliquÃ© pour vrai?

Yâ€™a quand mÃªme de cool avantages IMO :

- ConfidentialitÃ© VIP : ta data reste chez vous, zÃ©ro leak possible
sauf si tâ€™es du genre Ã  pas verrouiller ton Ã©cran

- Mode avion-*friendly* : pas besoin d'Internet pour que Ã§a roule

- CoÃ»t nul : une petite victoire dans lâ€™Ã©ternelle guerre contre les abonnements $

- RapiditÃ© : pas de latence rÃ©seau ni de files d'attente

- Full contrÃ´le : possibilitÃ© de customiser selon tes besoins

### Câ€™est quoi Ollama?

[Ollama](https://ollama.com/?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos) c'est comme ton gestionnaire de modÃ¨les AI local. Un peu comme Spotify pour des LLMs (Large Language Models). Tu tÃ©lÃ©charges l'app, puis t'as accÃ¨s Ã  un catalogue de modÃ¨les open source prÃªts Ã  rouler sur ta machine.

#### Quels modÃ¨les sont dispo?

Une sÃ©lection pas piquÃ©e des vers :

- Llama 3.2 (notre fidÃ¨le compagnon de test)

- DeepSeek-R1 (spÃ©cialisÃ© en raisonnementâ€”quand il crash pas ton laptop)

- Mistral

- Phi-4

- *30+ autres modÃ¨les*

**Pro tip** : check le nombre de [paramÃ¨tres](https://fr.wikipedia.org/wiki/Param%C3%A8tre_d%27un_mod%C3%A8le_de_langage?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos#:~:text=Dans%20le%20contexte%20des%20mod%C3%A8les,r%C3%A9seau%20de%20neurones%20du%20mod%C3%A8le.) avant d'installer. Mettons avec 8GB de RAM, tu peux en thÃ©orie rouler des modÃ¨les jusqu'Ã  ~7 milliards de paramÃ¨tres. Plus le nombre de paramÃ¨tres est Ã©levÃ©, plus [l'infÃ©rence](https://www.youtube.com/watch?v=XtT5i0ZeHHE&t=19s&utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos) sera lente et la consommation de mÃ©moire importante.

### Comment faire Ã§a?

Si jâ€™ai pu le faire en trente minutes, câ€™est sÃ»r que tu peux aussi. Lâ€™heure de gloire des n00bs est arrivÃ©e, je crois en toi.

Full vidÃ©o coming soon. Je lâ€™ai Ã©chappÃ© cÃ´tÃ© dÃ©lÃ©gation. Mais un bon leader prend la responsabilitÃ©, *you know*. Vu que je suis 80% un bon leader, vous pouvez donc blÃ¢mer Meto Ã  20%.

**Ã‰tape 1 : Installe Ollama**

1. Rends-toi sur [Ollama](https://ollama.com/?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos) et tÃ©lÃ©charge la version pour Mac (si tâ€™es sur Mac, *of course*).

2. Ouvre le fichier tÃ©lÃ©chargÃ© et dÃ©place Ollama dans le dossier **Applications**.

3. Lance Ollama. Lâ€™icÃ´ne devrait apparaÃ®tre en haut Ã  droite de ton Ã©cran.

4. Suis les instructions pour installer la ligne de commande via ton **Terminal** si demandÃ©.

si tâ€™as jamais ouvert ton Terminal sur Mac, Ã§a ressemble Ã  Ã§a!

**Ã‰tape 2 : Installe pinokio**

1. Va sur [pinokio](https://pinokio.computer?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos) et tÃ©lÃ©charge la version pour Mac.

2. Ouvre le fichier .dmg et dÃ©place pinokio dans le dossier **Applications**.

3. Ouvre pinokio. Si tu reÃ§ois un message de sÃ©curitÃ©, va dans **PrÃ©fÃ©rences SystÃ¨me > SÃ©curitÃ© et ConfidentialitÃ©** et autorise lâ€™application.

4. Dans pinokio, utilise lâ€™outil **Sentinel** pour retirer pinokio de la â€œquarantaineâ€ macOS si nÃ©cessaire.

**Ã‰tape 3 : Installe Open WebUI via pinokio**

1. Dans pinokio, cherche **Open WebUI** dans le *marketplace* de scripts.

2. Clique sur **Download**, puis sur **Install**. pinokio gÃ©rera automatiquement lâ€™installation des dÃ©pendances.

3. Une fois l'installation terminÃ©e, ouvre **Open WebUI** via pinokio. Ã‡a lancera une interface similaire Ã  ChatGPT dans ton navigateur, accessible via un lien **localhost**.

**Ã‰tape 4 : TÃ©lÃ©charge et utilise un LLM open source**

1. Ouvre Ollama et tÃ©lÃ©charge un modÃ¨le compatible avec ta RAM.

2. Exemple de commande dans le terminal pour tÃ©lÃ©charger un modÃ¨le :

1. Une fois le modÃ¨le tÃ©lÃ©chargÃ©, il apparaÃ®tra dans **Open WebUI**. Tu pourras choisir le modÃ¨le dans lâ€™interface et commencer Ã  interagir avec.

**Ã‰tape 5 : Change de modÃ¨le dans Open WebUI**

1. Pour ajouter un autre modÃ¨le, retourne sur le site dâ€™Ollama et trouve un modÃ¨le qui t'intÃ©resse (comme [DeepSeek-R1](https://api-docs.deepseek.com/news/news250120?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos) pour le raisonnement).

2. Utilise la commande suivante pour tÃ©lÃ©charger un nouveau modÃ¨le :

1. Le nouveau modÃ¨le sera disponible dans **Open WebUI**, oÃ¹ tu pourras alterner entre diffÃ©rents modÃ¨les dans une mÃªme conversation.

#### Conseils et idÃ©es :

- Les modÃ¨les plus lourds (plus de 7B paramÃ¨tres) peuvent Ãªtre lents ou causer des crashs si tu n'as pas assez de RAM.

- Open WebUI permet certaines fonctions comme la recherche web, mais tu devras configurer des clÃ©s API pour des services comme Google ou DuckDuckGo.

- Teste diffÃ©rents modÃ¨les selon tes besoins (raisonnement, code, crÃ©ativitÃ©).

- Explore l'app [Apollo AI](https://apps.apple.com/ca/app/apollo-ai-private-local-ai/id6448019325?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos) sur iOS pour une expÃ©rience similaire mobile.

**FÃ©licitations !** Tu peux maintenant utiliser des LLM open source localement sur ton ordinateur sans dÃ©pendre de services cloud payants qui espionnent tes recettes de cuisine.

Ta machine roule dans le tapis? *Good news*! Jumelle Ã§a Ã  un minimum de coton ouatÃ©s et bas de laine et tu peux maintenant rÃ©duire ton bill dâ€™Hydro de 20%.

*Thank me later*Â ğŸ”¥

â€”

Quelque chose Ã  ajouter? *Good*. Laisse un commentaire ou rÃ©ponds Ã  ce courriel direct.

Cheers,

[Frank](https://www.linkedin.com/in/frankln/?utm_source=saaspasse.beehiiv.com&utm_medium=newsletter&utm_campaign=un-lama-et-deux-bozos)Â ğŸ’œ